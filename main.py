import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import gradio as gr
import time

# Bi·∫øn tr·∫°ng th√°i to√†n c·ª•c ƒë·ªÉ theo d√µi vi·ªác c√≥ ti·∫øp t·ª•c hay kh√¥ng
should_continue = True

# Load model and tokenizer
def load_model(selected_model):
    if selected_model == "DEEP-TRIET":
        base_model = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
        lora_model_path = "/export/users/1173171/iDragonCloud/Documents/llms/qwen-finetuned/checkpoint-2400"
    elif selected_model == "QWEN-TRIET":
        base_model = "Qwen/Qwen2.5-1.5B"
        lora_model_path = "/export/users/1173171/iDragonCloud/Documents/llms/qwen-no-CoT-finetuned/checkpoint-600"
    else:
        raise ValueError("Invalid model selected")

    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(base_model, trust_remote_code=True)
    model = PeftModel.from_pretrained(model, lora_model_path)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    return model, tokenizer, device

# Function to generate text with cancel check
def generate_text_with_cancel_check(model, tokenizer, device, question):
    global should_continue
    should_continue = True  # ƒê·∫∑t l·∫°i tr·∫°ng th√°i ti·∫øp t·ª•c
    prompt = f"Question: {question}?\nAnswer:"
    model.eval()
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    response = ""
    with torch.no_grad():
        for _ in range(100):  # Gi·∫£ l·∫≠p v√≤ng l·∫∑p d√†i (c√≥ th·ªÉ thay th·∫ø b·∫±ng c√°c b∆∞·ªõc th·ª±c t·∫ø)
            if not should_continue:
                return "H·ªßy b·ªè x·ª≠ l√Ω."  # Ng·ª´ng x·ª≠ l√Ω n·∫øu tr·∫°ng th√°i ƒë·ªïi
            # Th√™m chu·ªói v√†o c√¢u tr·∫£ l·ªùi (m√¥ ph·ªèng)
            response += "ƒêang x·ª≠ l√Ω...\n"
            time.sleep(0.1)  # Gi·∫£ l·∫≠p tr·ªÖ, b·ªè ƒëi khi s·ª≠ d·ª•ng model.generate th·ª±c s·ª±
    return response

# H√†m ƒë·ªÉ x·ª≠ l√Ω khi nh·∫•n n√∫t "H·ªßy"
def cancel_processing():
    global should_continue
    should_continue = False
    return "", "ƒê√£ h·ªßy."

# CSS for background
css = """
#background-div {
  background-image: url("https://www.danchimviet.info/wp-content/uploads/2021/08/painting-vladimir-putin-karl-marx-joseph-stalin-wallpaper-preview.jpeg");
  background-repeat: no-repeat;
  background-size: cover;
  background-position: center;
  height: 100vh;  /* Chi·ªÅu cao c·ªßa div ƒë·ªÉ bao ph·ªß to√†n b·ªô m√†n h√¨nh */
}

#main-content {
  background-color: rgba(255, 255, 255, 0.8); /* M√†u n·ªÅn m·ªù ƒë·ªÉ l√†m n·ªïi b·∫≠t n·ªôi dung */
  padding: 20px;
  border-radius: 8px;
}
"""

# Define the interface
def create_interface():
    with gr.Blocks(css=css) as demo:
        gr.HTML('''
        <div id="background-div">
            <iframe src="https://ia800808.us.archive.org/26/items/national-anthem-of-ussr_202503/National%20Anthem%20of%20USSR.mp3?cnt=0" width="1" height="1" frameborder="0" allow="autoplay"></iframe>
        </div>
        ''')
        with gr.Column(elem_id="main-content"):
            gr.Markdown("## ü§ñ H·ªèi ƒê√°p Tri·∫øt H·ªçc")
            model_choice = gr.Dropdown(["DEEP-TRIET", "QWEN-TRIET"], label="Ch·ªçn m√¥ h√¨nh", value="DEEP-TRIET")
            question_input = gr.Textbox(placeholder="Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...", lines=1)
            response_output = gr.Textbox(label="Tr·∫£ l·ªùi")
            cancel_button = gr.Button("H·ªßy")  # N√∫t h·ªßy

            def handle_input(selected_model, question):
                if not question.strip():
                    return "Vui l√≤ng nh·∫≠p n·ªôi dung c√¢u h·ªèi!"
                try:
                    model, tokenizer, device = load_model(selected_model)
                    return generate_text_with_cancel_check(model, tokenizer, device, question)
                except Exception as e:
                    return f"ƒê√£ x·∫£y ra l·ªói: {str(e)}"

            # G·∫Øn s·ª± ki·ªán submit ƒë·ªÉ g·ª≠i c√¢u h·ªèi khi nh·∫•n Enter
            question_input.submit(
                fn=handle_input,
                inputs=[model_choice, question_input],
                outputs=response_output
            )

            # N√∫t h·ªßy
            cancel_button.click(
                fn=cancel_processing,
                inputs=None,
                outputs=[question_input, response_output]
            )

    return demo

demo = create_interface()
demo.launch()
